{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class11 - Natural Lanugage Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://juniorworld.github.io/python-workshop/img/NLP_.png\" width=\"600px\" height=\"400px\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demonstrate how to go through these five steps for English and Chinese texts respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. First-run Data Cleaning\n",
    "- Main task: convert the case, remove punctuations and special characters like hashtags, hyperlinks\n",
    "- Use Regular Expression for Pattern Matching\n",
    "- Convert the case: `.lower()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression Cheat Sheet\n",
    "1. `.`: Wildcard, any character\n",
    "2. `[abc]`: Range (a or b or c)\n",
    "    - \\: escape following special characters\n",
    "    - special characters that need to be escaped: `^ [ . $ { * ( \\ + ) | ? < >`\n",
    "3. `[^abc]`: Reverse range, Not (a or b or c)\n",
    "4. Quantifier:\n",
    "    - `*` at least 0 times.\n",
    "    - `+` at least 1 times.\n",
    "    - `?` at most 1 times.\n",
    "    - `{n}`: Exactly n times. \n",
    "    - `{n,}`: At least n times\n",
    "    - `{n,m}`: n-m times (m>n)\n",
    "5. Character class:\n",
    "    - `\\c`: Control character, such as line break, tab\n",
    "    - `\\s`: White space, `\\S`: Not white space\n",
    "    - `\\d`: Digit, `\\D`: Not digit\n",
    "    - `\\w`: Word, `\\W`: Not word\n",
    "    - `[:alnum:]`: Digits and letters = [0-9a-zA-Z]\n",
    "6. Location:\n",
    "    - `^` the start of the string\n",
    "    - `$` the end of the string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Substrings\n",
    "- `re.sub(pattern, new_string, original_string)`\n",
    "\n",
    "### Find All Substrings That Match the Pattern\n",
    "- `re.findall(re_pattern,string)`\n",
    "- result is a **LIST** of match substrings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet='@JerryNadler admits on #CNN they have no proof of Obstruction by @realDonaldTrump its just his \"personal opinion\" Meet the new #WitchHunt Same as the old #WitchHunt cc @DonaldJTrumpJr'\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all hashtags\n",
    "re.sub('#[^ ]+','',tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all hashtags\n",
    "re.findall('#[^ ]+',tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all mentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove hyperlinks\n",
    "tweet2='Read this article: https://goo.gl/rwGHTP Tesla’s abrupt shift to online-only car sales, after racing to open stores, battered its share price and raised questions about its future.'\n",
    "tweet3=re.sub('your_code_here','',tweet2)\n",
    "tweet3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all hyperlinks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non-word punctuations\n",
    "re.sub('[\\W]+',' ',tweet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also works for Chinese text\n",
    "re.sub('[\\W]+',' ','普京表示，歡迎中方在化解危機中的建設性角色！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**<b>Reminder</b>** By removing punctuations, you will also loose the full stops and line breaks. You won't be able to analyze sentence and paragraph structure. ONLY USE [\\W]+ when the paragraph/sentence structure is not important for you.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove spaces and line breaks at the beginning or the end of the sentence\n",
    "text=' 普京表示 歡迎中方在化解危機中的建設性角色 '\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data_cleaning() function to convert letter case, remove punctuations, numbers, mentions, hashtags and hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub('[0-9]+','',text)\n",
    "    text=re.sub('@[^ ]+','',text)\n",
    "    text=re.sub('#[^ ]+','',text)\n",
    "    text=re.sub('https:[^ ]+','',text)\n",
    "    text=re.sub('[\\W]+',' ',text)\n",
    "    text=text.strip()\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your function with a post from @realDonaldTrump\n",
    "a='@seanhannity “We the people will now be subjected to the biggest display of modern day McCarthyism....which is the widest fishing net expedition....every aspect of the presidents life....all in order to get power back so they can institute Socialism.” https://t.co/izb2tTrINB'\n",
    "data_cleaning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "- Definition: tokenization is a process of splitting sentences/paragraphs/documents into a set of words.\n",
    "- Differences in Languages:\n",
    "    - English: **words** are naturally separated with spaces\n",
    "    - Korean: **phrases** are naturally separated with spaces\n",
    "        - konlpy (http://konlpy.org/)\n",
    "    - Chinese/Japanese: **no spaces** in text\n",
    "        - Chinese: jieba (https://github.com/fxsjy/jieba)\n",
    "        - Japanese: jNlp (https://github.com/kevincobain2000/jProcessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize English Text: Hunt for Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the following sentence into words\n",
    "sentence='Mr. Zuckerberg, who runs Facebook, Instagram, WhatsApp and Messenger, on Wednesday expressed his intentions to change the essential nature of social media. Instead of encouraging public posts, he said he would focus on private and encrypted communications, in which users message mostly smaller groups of people they know. Unlike publicly shared posts that are kept as users’ permanent records, the communications could also be deleted after a certain period of time.'\n",
    "sentence=data_cleaning(sentence)\n",
    "words=sentence.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Second-run Data Cleaning\n",
    "- Main taks: remove stop words, stem/lemmatize words\n",
    "\n",
    "### 3.1 Remove stop words\n",
    "<font style=\"color:red\">English stop words file: https://juniorworld.github.io/python-workshop/doc/stop_words_eng.txt</font><br>\n",
    "Stop words are useless for understanding text.<br>\n",
    "- English: at, in, on, for, of, a, an, the...<br>\n",
    "- Chinese: 的，地，得，了.<br>\n",
    "\n",
    "However, the combination of 得了 is not a stop word.<br>\n",
    "-> Absolute Match\n",
    "\n",
    "Solution: Membership check - Check whether a word is in the predefined stopword list\n",
    ">```python\n",
    "x in [word1, word2, word3]\n",
    "x not in [word1, word2, word3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a' in ['a','b','c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a' in ['aa','b','c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a' not in ['aa','b','c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=open(\"path_to_stopword_file\",'r')\n",
    "stopwords=[i.strip() for i in stopwords.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go over every word in the list and check if it is a stop word\n",
    "#if not, add it to new list words_rm\n",
    "words_rm=[]\n",
    "for word in words:\n",
    "    #write your code here\n",
    "    \n",
    "words_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a user function called remove_stopwords()\n",
    "#Function Input: token list\n",
    "#Function Output: token list without stop words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Stem/Lemmatize Words\n",
    "- Use external package `gensim` and `nltk` to achieve word stemming and lemmatization.\n",
    "- Words can have many variants and derivations. The goal of stemming and lemmatization is to convert the words back into their roots.\n",
    "- **Stem**: Crude. Trim off the suffixes and derivational affixes of words without knowledge of the context. Results are called stems.\n",
    "    - Advantage: Simple, Fast\n",
    "    - Disadvantage: Less accurate. Results are incomplete word roots. Cannot identify complex, context-based variants, such as comparative.\n",
    "    - Example: women -> women, apples -> appl, likely -> like, better -> better\n",
    "    >```python\n",
    "    from gensim.parsing.porter import PorterStemmer\n",
    "    Stemmer=PorterStemmer()\n",
    "    Stemmer.stem('word')\n",
    "    Stemmer.stem_documents(list_of_words)\n",
    "    ```\n",
    "- **Lemmatization**: Consider the context (part of speech of words) and converts the word to its meaningful base form, which is called Lemma.\n",
    "    - Advantage: Accurate and Contextualized. Results are meaningful, complete words.\n",
    "    - Disadvantage: Computationally Costly. Slow. Need to specify the part of speech. NLTK only supports lemmatizing nouns, adjs, and verbs.\n",
    "    - Example: women -> woman, apples -> appl, likely -> likely, is -> be, better -> good\n",
    "    >```python\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    nltk.download('wordnet')\n",
    "    Lemmatizer=WordNetLemmatizer()\n",
    "    Lemmatizer.lemmatize('word','pos')\n",
    "    #pos = part of speech, 'n'=noun [default], 'a'=adjective, 'v'=verb\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gensim\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "Stemmer=PorterStemmer()\n",
    "Stemmer.stem('walking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Stemmer.stem('cats'),Stemmer.stem('apples'),Stemmer.stem('women'),Stemmer.stem('likely'),Stemmer.stem('is'),Stemmer.stem('better'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stemmer.stem_documents([\"cats\", \"apples\",\"women\",\"likely\",\"is\",\"better\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stem the words\n",
    "words_stem=Stemmer.stem_documents(words_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "Lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Lemmatizer.lemmatize('cats'),Lemmatizer.lemmatize('apples'),Lemmatizer.lemmatize('women'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Lemmatizer.lemmatize('likely','a'),Lemmatizer.lemmatize('is','v'),Lemmatizer.lemmatize('better','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**<b>Reminder</b>** NLTK's lemmatizer is better used together with its nltk.pos_tag() function.<br>\n",
    "    The resulting tags are annotated according to: <a href=\"https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html\">https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html</a><br>\n",
    "    POS of interest: Starting with 'N','J','V'</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.pos_tag(words_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(words_rm):\n",
    "    words_lemma=[]\n",
    "    for word,pos in nltk.pos_tag(words_rm):\n",
    "        if pos[0]=='N':\n",
    "            words_lemma.append(Lemmatizer.lemmatize(word))\n",
    "        elif pos[0]=='J':\n",
    "            words_lemma.append(Lemmatizer.lemmatize(word,'a'))\n",
    "        elif pos[0]=='V':\n",
    "            words_lemma.append(Lemmatizer.lemmatize(word,'v'))\n",
    "        else:\n",
    "            words_lemma.append(word)\n",
    "    return(words_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_lemma=lemmatization(words_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download file from: https://juniorworld.github.io/python-workshop/doc/trump_tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "table=pd.read_csv('https://juniorworld.github.io/python-workshop/doc/trump_tweets.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a program to:\n",
    "#Step 1: clean text\n",
    "#Step 2: tokenize text\n",
    "#Step 3: remove stop words from tokens\n",
    "#Step 4: lemmatize tokens\n",
    "#Step 5: save lemmas into csv file. tokens in the same tweet are saved at the same line\n",
    "#Step 6: combine all lemmas into a list\n",
    "lemmas_all=[]\n",
    "import csv\n",
    "file=open('trump_twitter_tokens.csv','w',newline='\\n')\n",
    "writer=csv.writer(file)\n",
    "for text in table['tweet']:\n",
    "    #Write your code here\n",
    "    \n",
    "    lemmas_all.extend()\n",
    "    writer.writerow()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_freq=pd.Series(lemmas_all).value_counts().reset_index()\n",
    "lemmas_freq.columns=['word','freq']\n",
    "lemmas_freq[['freq','word']].to_csv('trump_wordcloud.csv',sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUIZ\n",
    "\n",
    "https://www.menti.com/alhbfixudxfg\n",
    "\n",
    "<img src=\"https://juniorworld.github.io/python-workshop/img/Week%2011_NLP_quiz.png\" width=\"200\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud\n",
    "HTML5 Word Cloud: http://timc.idv.tw/wordcloud/#\n",
    "\n",
    "### Word Co-occurence Network\n",
    "Download Gephi from: https://gephi.org/users/download/ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract mentions from his tweets\n",
    "# Generate a Co-Mention Network\n",
    "# Where nodes represent twitter handlers and links represent co-mention relationship in tweets\n",
    "# Hint: \n",
    "# Step 1: Check if the post has more than one mentions\n",
    "# Step 2: If yes, save mentions within a post into a row\n",
    "file=open('trump_twitter_comentions.csv','w',newline='\\n')\n",
    "writer=csv.writer(file)\n",
    "for text in table['tweet']:\n",
    "    #Write your code here\n",
    "    \n",
    "    writer.writerow()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "- Word embedding is a technique to transform textual words into a numerical representation (word vector). Each word is mapped to one vector and this vector is trained to learn the syntactic and semantic relationships between words.\n",
    "    - e.g.: \"book\" (1,0,3), \"paper\" (1,0,4) -> synonyms are close in the word space\n",
    "- Implications:\n",
    "    - Find out synonyms in the corpus (a list of documents). A way to understand the structure of opinion expression.\n",
    "        - In Trump's opinions, CNN is a byword for enemy and dishonesty, while Clinton is a synonym for crook.\n",
    "    - Find out the equivalent mapping between words and concepts. It can understand the meaning of word combination by assuming the meaning of words is transmissive.\n",
    "        - \"queen\" is to \"women\" is what \"king\" is to \"men\"\n",
    "        - \"Beijing\" is to \"China\" is what \"Tokyo\" is to \"Japan\"\n",
    "        - Vectors are eligible for math operations, like + and -\n",
    "    - Find out the least similar (least possible) word in a sentence\n",
    "    - Evaluate the similarity (distance) between two or more sentences\n",
    "    - Evaluate the possibility of a sentence belonging to this corpus\n",
    "        - Corpus: A set of text documents\n",
    "\n",
    "<img src=\"https://www.usna.edu/Users/cs/nchamber/courses/nlp/f20/labs/lab5/wordgraph.png\" width=\"400\">\n",
    "Reference: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('trump_twitter_tokens.csv','r')\n",
    "reader = csv.reader(file)\n",
    "tokens=[]\n",
    "for row in reader:\n",
    "    tokens.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word document matrix\n",
    "#rows: documents\n",
    "#elements: words\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=1, workers=4, hs=1, negative=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10 synonyms most similar to a target word in the word space\n",
    "# Try: cnn, clinton, biden, election, china\n",
    "model.wv.most_similar('cnn', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get synonyms based on a target vector \n",
    "# Tariff is to china is what XX is to usa\n",
    "model.wv.most_similar(positive=['china','usa'], negative=['tariff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the least similar word in a list of words\n",
    "model.wv.doesnt_match(['china','usa','canada','cnn','mexico'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the possibility that Trump says such a sentence in his Twitter account\n",
    "model.score(['cnn is friend'.split(),\n",
    "             'cnn is enemy'.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(['china is friend'.split(),\n",
    "             'china is enemy'.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Predict rating of a review comment of Amazon Alexa\n",
    "1. Open this data frame: https://juniorworld.github.io/python-workshop/doc/amazon_alexa.tsv\n",
    "2. Train two Word2Vec models of word representations, for 5-star reviews and non-5-star reviews respectively\n",
    "3. Predict the rating of a comment, saying \"It doesn't answer me sometimes and give me some irrelevant answers quite often\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexa_reviews=pd.read_csv('https://juniorworld.github.io/python-workshop/doc/amazon_alexa.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexa_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many 5-star, 4-star, 3-star, 2-star, 1-star ratings?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_stars_comments=[]\n",
    "for text in : #fill in your code\n",
    "    text=data_cleaning(text)\n",
    "    #write your code here\n",
    "    \n",
    "    \n",
    "    five_stars_comments.append(words_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_five_stars_comments=[]\n",
    "for text in : #fill in your code\n",
    "    text=data_cleaning(text)\n",
    "    #write your code here\n",
    "    \n",
    "    non_five_stars_comments.append(words_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star_model=Word2Vec(sentences=five_stars_comments, vector_size=100, window=5, min_count=1, workers=4, hs=1, negative=0)\n",
    "non_five_star_model=Word2Vec(sentences=non_five_stars_comments, vector_size=100, window=5, min_count=1, workers=4, hs=1, negative=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star_model.score([data_cleaning(\"It doesn't answer me sometimes and give me some irrelevant answers quite often\").split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_five_star_model.score([data_cleaning(\"It doesn't answer me sometimes and give me some irrelevant answers quite often\").split()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
