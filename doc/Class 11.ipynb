{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Association Test (WEAT)\n",
    "\n",
    "- Evaluate Implicit Association between social categories and attributes.\n",
    "  - Conventional Implicit Association Test (IAT) is a well-known pyschological assessment intended to detect subconscious associations between mental representations of objects (concepts) in memory.\n",
    "  - Easier pairings that result in faster responses are interpreted as more strongly associated in memory.\n",
    "- WEAT: Comparing the association between two sets of target words and two sets of attributes.\r\n",
    "  - Target words: Social categories, such as female/male and races.\r\n",
    "  - Attribute words: Merits (honest), characteristics (pleasant), and occupations (programmers and nurses).\r\n",
    "  - Association: Cosine similarity\r\n",
    "  - Statistical test: Independent/Pairwise T Test\r\n",
    "\r\n",
    "_Reference: Caliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like moral choices. Science, 356(April), 183–186. https://doi.org/10.1145/3306618.3314267_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Bias\n",
    "1. Prepare two lists of female-typed and male-typed names\n",
    "2. Target two gender-typed attributes, i.e. family vs career\n",
    "3. Compile a sufficient number of keywords that are representative of the attribute, i.e. family-related words vs career-related words\n",
    "4. Calculate the average cosine similarity (association) between a specific name and family/career words\n",
    "   - Word Embedding Models\n",
    "   - Extract word vectors associated with names and attribute words\n",
    "   - Integrate them into a big matrix where rows represent names and columns represent latent dimensions\n",
    "6. Substract career and family association for a given name -> Relative strength of **family association**, in contrast to the career association (to what extent the name's family association outsizes its career association)\n",
    "7. Compare the relative family association of female/male groups -> T test\n",
    "   - `sklearn.metrics.pairwise.cosine_similarity(X, Y)` Compute cosine similarity between all possible combinations of samples in X and Y.\n",
    "\n",
    "Download this file: https://juniorworld.github.io/python-workshop/doc/weat_gender.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "weat_dict = json.loads(open('./doc/weat_gender.json','r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each dictionary element contains a list of words associated with a certain social category (key)\n",
    "weat_dict['Female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall 19 female-typed names/pronouns are selected.\n",
    "len(weat_dict['Female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weat_dict['Family']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections.abc\n",
    "#Hyper needs the four following aliases to be done manually.\n",
    "collections.Iterable = collections.abc.Iterable\n",
    "collections.Mapping = collections.abc.Mapping\n",
    "collections.MutableSet = collections.abc.MutableSet\n",
    "collections.MutableMapping = collections.abc.MutableMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all available models in gensim-data\n",
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format: **Model_Name**-**Training_source**-**Dimensions**\r\n",
    "\r\n",
    "Model name:\r\n",
    "1. `fasttext` invented at Facebook\r\n",
    "2. `word2vec` invented at Google\r\n",
    "3. `GloVe` invented at Stanford\r\n",
    "\r\n",
    "Training sources:\r\n",
    "1. Twitter\r\n",
    "2. Google News\r\n",
    "3. Wikipedia articles\r\n",
    "\r\n",
    "Dimensions: Vector Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe model trained on the Wiki articles with 200 dimensions\n",
    "glove = gensim.downloader.load('glove-wiki-gigaword-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a word exists in the pretrained word embedding\n",
    "'she' in glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the word vector for \"adam\"\n",
    "glove['she']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yuner does not exist in the pretrained word embedding \n",
    "'yuner' in glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install sklearn if you haven't done so\n",
    "! pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_vectors(words, word_embedding):\n",
    "    vectors = np.empty(shape=(0,200)) #initialize an empty array (a data type supported by numpy, equivalent to list)\n",
    "    for i in words:\n",
    "      if i in word_embedding:\n",
    "        vectors = np.vstack([vectors,word_embedding[i]]) #stack arrays vertically\n",
    "    return(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert female, male, family, and career words into vectors\n",
    "Female_vectors = \n",
    "Male_vectors = \n",
    "Family_vectors = \n",
    "Career_vectors = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Female_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Family_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Female_Family_association = cosine_similarity(Female_vectors, Family_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Female_Family_association.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Female_Family_mean = np.mean(Female_Family_association, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Female_Family_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Female_Family_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Female_Career_association = cosine_similarity(Female_vectors, Career_vectors)\n",
    "Female_Career_mean = np.mean(Female_Career_association, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Female_Career_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this block of scripts mean?\n",
    "for i in range(len(Female_Family_mean)):\n",
    "    print(weat_dict['Female'][i],Female_Family_mean[i]-Female_Career_mean[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which name is the most family-oriented name in the current word embedding model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Male_Family_association = cosine_similarity(Male_vectors, Family_vectors)\n",
    "Male_Family_mean = np.mean(Male_Family_association, axis=1)\n",
    "Male_Career_association = cosine_similarity(Male_vectors, Career_vectors)\n",
    "Male_Career_mean = np.mean(Male_Career_association, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this block of scripts mean?\n",
    "for i in range(len(Male_Family_mean)):\n",
    "    print(weat_dict['Male'][i],Male_Family_mean[i]-Male_Career_mean[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student's T-test\n",
    "- Student's t-test is a statistical test used to test whether the difference between two groups is statistically significant or not.\n",
    "- https://www.youtube.com/watch?v=pTmLQvMM-1M\n",
    "- Different variants:\n",
    "  - Independent t-test: Two independent groups\n",
    "  - Paired sample t-test: Two matched groups associated with identical or similar subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Female_association = Female_Family_mean-Female_Career_mean\n",
    "Male_association = Male_Family_mean-Male_Career_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare group means\n",
    "np.mean(Female_association)>np.mean(Male_association)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.ttest_ind(Female_association, Male_association)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excersie\n",
    "Write a program to evaluate the association between genders and art/science. <br>\n",
    "In this word embedding, is there a stronger association between females and art compared to males, as opposed to science?\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Special about Chinese Language?\n",
    "1. First-run Cleaning: No need to convert <font style=\"color: blue\">letter case</font>\n",
    "    - data_cleaning() solely for chinese language will be slighter than that for english, without a line of text.lower()\n",
    "    - However, we typically will use english-version data_cleaning(), in case there are some english character in the text\n",
    "2. Tokenization: No <font style=\"color: blue\">natural deliminator</font>, like the space in Eng. Need to rely on language model to split text into words.\n",
    "3. Second-run Cleanig: No need to <font style=\"color: blue\">stem/lemmatize</font> words\n",
    "4. Vectorization: Identical to English\n",
    "\n",
    "#### 1. First-run Data Cleaning\n",
    "- Main task: Remove punctuations and special characters like hashtags, hyperlinks\n",
    "- Use Regular Expression for Pattern Matching\n",
    "- No need to convert cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also works for Chinese text\n",
    "import re\n",
    "re.sub('[\\W]+',' ','普京表示，歡迎中方在化解危機中的建設性角色！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub('[0-9]+','',text)\n",
    "    text=re.sub('@[^ ]+','',text)\n",
    "    text=re.sub('#[^ ]+','',text)\n",
    "    text=re.sub('https:[^ ]+','',text)\n",
    "    text=re.sub('[\\W]+',' ',text)\n",
    "    text=text.strip()\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the data_cleaning() function with a Weibo post\n",
    "a=\"各國應轟炸俄羅斯境內“暈輸線”……當年“炮擊金門”很久，最後因美國切斷了“廈門車站”運輸線，炮擊金門才止。（而不應去烏克蘭建軍工廠：俄會集中火力轟炸。）@美国驻华大使馆 @英國駐華使館 @歐盟在中國 @烏克蘭信使\"\n",
    "data_cleaning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a package package \"jieba\" to tokenize Chinese text.<br>\n",
    "<br>\n",
    "**Why jieba?**\n",
    "- It adopts a hybrid method combining both statistical/probabilistic inference and pattern matching based on dictionary. \n",
    "    - capable to recognize words existing in the pre-defined dictionary\n",
    "    - capable to find new words.\n",
    "- Two dictionaries:\n",
    "    - System dictionary\n",
    "        - Simplied Chinese\n",
    "        - Simplied+Traditional Chinese\n",
    "    - User dictionary\n",
    "- Syntax:\n",
    ">```python\n",
    "jieba.cut(sentence) #result is a list of words\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(jieba.cut('你好，这是一个简单的句子。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it can segment tradional Chinese text by using statistical inference method.\n",
    "list(jieba.cut('你好，這是一個簡單的句子。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#however, statistical inference is not always perfect.\n",
    "list(jieba.cut('談判擱置，工會號召靜坐。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(jieba.cut('谈判搁置，工会号召静坐。'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we improve statistical inference for the tokenization?<br>\n",
    "**Human in the loop**: Provide human-defined dictionary to constrain and fine-tune the statistical inference.\n",
    "##### Solution: Configurate Dictionaries\n",
    "- Two types of dictionaries:\n",
    "    1. System dictionary: General purpose\n",
    "    2. User dictionary: Special context, e.g. dictionaries for emotion, incivility, war\n",
    "- How does the dictionary look like?\n",
    "    - Don't confuse with the data type “dictionary”\n",
    "    - Dictionary is a plain text file\n",
    "    - One line one keyword, similar to the stopword list/file\n",
    "    - [Optional] Words might also be weighted, carrying with a number/decimal suggestive of the importance of the words\n",
    "\n",
    ">```\n",
    ">#Way 1: no weight: all words are created equal\n",
    "China\n",
    "People's Republic of China\n",
    "China Central Television\n",
    "\n",
    ">```\n",
    "#Way 2: with weights: words are treated unequally. Higher weight, Higher priority\n",
    "China,3\n",
    "People's Republic of China, 4\n",
    "China Central Television,4\n",
    "\n",
    "\n",
    "- In jieba, you can load dictionaries using the following syntaxes:\n",
    ">```python\n",
    "jieba.set_dictionary(\"path_of_system_dict\") \n",
    "jieba.load_userdict(\"path_of_user_dict\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better segment traditional Chinese text, we need to upgrade system dictionary to include traditional Chinese words.<br>\n",
    "Download the system dictionary from this link:https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load traditional Chinese system dictionary\n",
    "jieba.set_dictionary('./doc/dict.txt.big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try tokenizing this sentence again\n",
    "list(jieba.cut('談判擱置，工會號召靜坐。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some names and special terminologies cannot be properly identified.\n",
    "print(list(jieba.cut('中央上周二向特首林鄭月娥發公函'))) #very long name\n",
    "print(list(jieba.cut('蔡英文日前表示希望與日本舉行安保對話'))) #names including frequently used words\n",
    "print(list(jieba.cut('高雄市長韓國瑜本月稍後訪問港澳深圳廈門四市'))) #names including frequently used words\n",
    "print(list(jieba.cut('汶萊的全稱為汶萊達魯薩蘭國。'))) #special terminologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a for loop to build your user dictionary (time-consuming)\n",
    "file=open('user_dict.txt','w',encoding='utf-8')\n",
    "keywords=['林鄭月娥','蔡英文','韓國瑜','汶萊達魯薩蘭國']\n",
    "#Write your loop here\n",
    "for keyword in keywords:\n",
    "    file.write(keyword+'\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use your user dictionary\n",
    "jieba.load_userdict('user_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After loading user dictionary:\n",
    "print(list(jieba.cut('中央上周二向特首林鄭月娥發公函'))) #very long name\n",
    "print(list(jieba.cut('蔡英文日前表示希望與日本舉行安保對話'))) #names including frequently used words\n",
    "print(list(jieba.cut('高雄市長韓國瑜本月稍後訪問港澳深圳廈門四市'))) #names including frequently used words\n",
    "print(list(jieba.cut('汶萊的全稱為汶萊達魯薩蘭國。'))) #terminologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chinese stop words file: https://juniorworld.github.io/python-workshop/doc/stop_words_chi.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stop word list\n",
    "file_chi=open('./doc/stop_words_chi.txt','r',encoding='utf-8')\n",
    "stopwords=[i.strip() for i in file_chi.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stop_words) #much longer and detailed than english stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have a look at the dictionary\n",
    "stopwords[34:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    global stopwords\n",
    "    words_rm=[]\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            words_rm.append(word)\n",
    "    return(words_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph='Facebook CEO 馬克·朱克伯格（Mark Zuckerberg）週三發布了一篇長文，闡述了要將 Facebook 打造成「以隱私為中心的平台」的願景，並表示將打通 Messenger、Instagram 和 WhatsApp 用戶之間的交流阻礙。朱克伯格表示，他相信未來人們的溝通行為會更多轉向私人加密服務，也未必希望他們分享的所有內容都被永遠保存在互聯網上——後者對於每個人來說，既可能是財富，也可能是負擔。因此，儘管 Facebook 長期以來專注於打造開放、分享的社區平台，但他認為，以隱私為中心的通信平台會比當今的開放平台更加重要。'\n",
    "text_clean=data_cleaning(paragraph)\n",
    "words=jieba.cut(text_clean)\n",
    "words_rm=remove_stopwords(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Containing too many meaningless whitespaces\n",
    "words_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add whitespace to the stopword lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun the codes\n",
    "text_clean=data_cleaning(paragraph)\n",
    "words=jieba.cut(text_clean)\n",
    "words_rm=remove_stopwords(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#count word frequency\n",
    "pd.Series(words_rm).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(words_rm).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:blue'>Exercise</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the 10 fade-in and fade-out words in speeches.<br>\n",
    "The magnitude of difference is measured by the change in their relative frequencies:<br>\n",
    "<p style='text-align:center;font-size:15px;'>Relative Freq (RF) = word frequency / sum of word frequencies</p>\n",
    "<p style='text-align:center;font-size:15px;'>Difference = RF<font size='2px'>2019</font> - RF<font size='2px'>2009</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options:<br>\n",
    "- Chinese: Annual government work reports, <a href=\"https://juniorworld.github.io/python-workshop/doc/2019_Government_Work_Report.txt\">2019</a> vs <a href=\"https://juniorworld.github.io/python-workshop/doc/2009_Government_Work_Report.txt\">2009</a>\n",
    "- English: State of the Union address, <a href=\"https://juniorworld.github.io/python-workshop/doc/2019_SoU.txt\">2019</a> vs <a href=\"https://juniorworld.github.io/python-workshop/doc/2009_SoU.txt\">2009</a><br>\n",
    "\n",
    "*Hint:*<br>\n",
    "*1. Use `pd.concat([df1,df2],axis=1)` to combine two dataframes by columns and `pd.concat([df1,df2],axis=0)` to combine two dataframes by rows*<br>\n",
    "*2. Use `df[column_name].value_counts()` to count the items in a column.*<br>\n",
    "*3. Use `df.sort_values(column_name,ascending=True)` to sort a certain column. To get a reversed list, you can set ascending=False* <br>\n",
    "*4. Use `df.fillna(0)` to replace NAN value with 0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Chinese files\n",
    "Chi_file_2019=open('./doc/2019_Government_Work_Report.txt','r',encoding='utf-8')\n",
    "Chi_file_2009=open('./doc/2009_Government_Work_Report.txt','r',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHI 2019\n",
    "#Step1: Clean text\n",
    "#Step2: Tokenize text\n",
    "#Step3: Remove stopwords\n",
    "#Step4: Add the current word list to Chi_words_2019\n",
    "#---------------------------------------------------\n",
    "Chi_words_2019=[]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHI 2009\n",
    "Chi_words_2009=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count relative word frequencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine relative_freq_2019 and relative_freq_2009 into relative_freq, using pd.concat() function\n",
    "relative_freq = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill out missing values with 0\n",
    "relative_freq=relative_freq.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the frequency difference\n",
    "relative_freq['diff']="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change column names\n",
    "relative_freq.columns="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort table by column 'diff'\n",
    "#Fade in words: words that are more common in 2019 report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fade out words: words that are more common in 2009 report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon & Team Project\n",
    "\n",
    "1. Team Size = 4 ppl\n",
    "2. Random Grouping or Self Grouping?\n",
    "3. Format:\n",
    "   - 3-hour Hackathon: Submit a draft (no word limits) explaining your team plan, division of labour, and the preliminary results you have derived within the 3 hours. You will also need to turn in your jupyter notebook.\n",
    "   - 1-week Extended work: Extend the draft into a full paper with no less than 3200 words.\n",
    "4. Datasets:\n",
    "   - Social media postings\n",
    "   - News articles\n",
    "   - Reviews/Comments\n",
    "   - You have leeway to add new dataset to your study\n",
    "5. Grade breakdowns:\n",
    "   - Hackathon: 10 points\n",
    "     - Group grading: 5 points\n",
    "     - Individual grading: 5 points\n",
    "   - Extended paper: 10 points\n",
    "     - Group grading: 5 points\n",
    "     - Individual grading: 5 points\n",
    "   - Peer Evaluation: 3 points\n",
    "     - Evaluations lacking variation, such as exclusively consisting of either 5-star or 1-star ratings, will result in disqualification.\n",
    "6. Evaluation criteria:\n",
    "   - Effort-based grading\n",
    "   - Sophistication/Richness of results\n",
    "   - Computional thinking & Code quality\n",
    "   - Storytelling skills: describe the data clearly, explain the rationale for the study, incorporate visualizations and statistics smartly, and derive insightful findings from the analyses\n",
    "   - Team work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
