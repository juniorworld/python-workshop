{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 9 - Natural Lanugage Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flowchart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://juniorworld.github.io/python-workshop/img/NLP_.png\" width=\"600px\" height=\"400px\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demonstrate how to go through these five steps for English and Chinese texts respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. First-run Data Cleaning\n",
    "- Main task: convert the case, remove punctuations and special characters like hashtags, hyperlinks\n",
    "- Use Regular Expression for Pattern Matching\n",
    "- Convert the case: `.lower()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expression Cheat Sheet\n",
    "1. `.`: Wildcard, any character\n",
    "2. `[abc]`: Group (a or b or c)\n",
    "    - \\\\: escape following special characters\n",
    "    - special characters that need to be escaped: `^ [ . $ { * ( \\ + ) | ? < >`\n",
    "3. `[^abc]`: Reverse group, Not (a or b or c)\n",
    "4. Character class:\n",
    "    - `\\w`: Word (incl. chars and numbers), `\\W`: Non-word\n",
    "    - `\\c`: Control character, such as line break, tab\n",
    "    - `\\s`: White space, `\\S`: Not white space\n",
    "    - `\\d`: Digit = [0-9], `\\D`: Not digit = [^0-9]\n",
    "    - `[:alnum:]`: Digits and letters = [0-9a-zA-Z]\n",
    "\n",
    "5. Quantifier:\n",
    "    - `*` at least 0 times.\n",
    "    - `+` at least 1 times.\n",
    "    - `?` at most 1 times.\n",
    "    - `{n}`: Exactly n times. \n",
    "    - `{n,}`: At least n times\n",
    "    - `{n,m}`: n-m times (m>n)\n",
    "\n",
    "6. Location:\n",
    "   - `^` the start of the string\n",
    "   - `$` the end of the string\n",
    "     \n",
    "8. Logic operation: \n",
    "   - `|` or\n",
    "   - `&` and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Substrings\n",
    "- `re.sub(pattern, new_string, original_string)`\n",
    "\n",
    "### Find All Substrings\n",
    "- `re.findall(re_pattern,string)`\n",
    "- `()` Limited extraction. Specify the particular substrings that you want to extract.\n",
    "- result is a **LIST** of match substrings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet='@JerryNadler admits on #CNN they have no proof of Obstruction by @realDonaldTrump its just his \"personal opinion\" Meet the new #WitchHunt Same as the old #WitchHunt cc @DonaldJTrumpJr'\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all hashtags\n",
    "re.sub('#[^ ]+','',tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all hashtags\n",
    "re.findall('#([^ ]+)',tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all mentions\n",
    "re.findall(\"@[^ ]+\",tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many people does this tweet mention?\n",
    "len(re.findall(\"@[^ ]+\",tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove hyperlinks\n",
    "tweet2='Read this article: https://goo.gl/rwGHTP Tesla’s abrupt shift to online-only car sales, after racing to open stores, battered its share price and raised questions about its future.'\n",
    "tweet3=re.sub('https:[^ ]+','',tweet2)\n",
    "tweet3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all hyperlinks\n",
    "re.findall('http[^ ]+',tweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non-word punctuations\n",
    "re.sub('[\\W]+',' ',tweet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also works for Chinese text\n",
    "re.sub('[\\W]+',' ','普京表示，歡迎中方在化解危機中的建設性角色！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**<b>Reminder</b>** By removing punctuations, you will also loose the full stops and line breaks. You won't be able to analyze sentence and paragraph structure. So, [\\W]+ should ONLY BE USED WHEN the paragraph/sentence structure is not important for you.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove spaces and line breaks at the beginning or the end of the sentence\n",
    "text=' 普京表示\\n歡迎中方在化解危機中的建設性角色 '\n",
    "text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(\"^ | $\",\"\",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Functions\n",
    "\n",
    "- Function is a block of reusable codes. Annotation: y=f(x), where x is a list of input variables and y is a list of output variables.\n",
    "    - Terminology: input variables = <b>parameters</b>, output variables = <b>returned variables</b> and their actual values = <b>arguments</b>\n",
    "    - <b>Global vs Local</b>: function can create its local variables that are only used inside its boundary. Local variables can use same names as global variables without overriding their values.\n",
    "    - Format:\n",
    ">```python\n",
    "def function_name(input1[,input2,input3...]):\n",
    "        command line\n",
    "        return(output) \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The function of function is to transform x into y. Like a magic trick turning a girl into a tiger.\n",
    "\n",
    "<img src='img/week2-function.png' width='200px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mean() function without return values\n",
    "def mean(x):#x is a list of numbers\n",
    "    y = sum(x)/len(x)\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mean([1,2,3])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a data_cleaning() function to convert letter case, remove punctuations, numbers, mentions, hashtags and hyperlinks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub('[0-9]+','',text)\n",
    "    text=re.sub('@[^ ]+','',text)\n",
    "    text=re.sub('#[^ ]+','',text)\n",
    "    text=re.sub('https:[^ ]+','',text)\n",
    "    text=re.sub('[\\W]+',' ',text)\n",
    "    text=text.strip()\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your function with a post from @realDonaldTrump\n",
    "a='@seanhannity “We the people will now be subjected to the biggest display of modern day McCarthyism....which is the widest fishing net expedition....every aspect of the presidents life....all in order to get power back so they can institute Socialism.” https://t.co/izb2tTrINB'\n",
    "data_cleaning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization\n",
    "- Definition: tokenization is a process of splitting sentences/paragraphs/documents into a set of words.\n",
    "- Differences in Languages:\n",
    "    - English: **words** are naturally separated with spaces\n",
    "    - Korean: **phrases** are naturally separated with spaces\n",
    "        - konlpy (http://konlpy.org/)\n",
    "    - Chinese/Japanese: **no spaces** in text\n",
    "        - Chinese: jieba (https://github.com/fxsjy/jieba)\n",
    "        - Japanese: jNlp (https://github.com/kevincobain2000/jProcessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize English Text: Hunt for Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the following sentence into words\n",
    "sentence='Mr. Zuckerberg, who runs Facebook, Instagram, WhatsApp and Messenger, on Wednesday expressed his intentions to change the essential nature of social media. Instead of encouraging public posts, he said he would focus on private and encrypted communications, in which users message mostly smaller groups of people they know. Unlike publicly shared posts that are kept as users’ permanent records, the communications could also be deleted after a certain period of time.'\n",
    "sentence=data_cleaning(sentence)\n",
    "words=sentence.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Second-run Data Cleaning\n",
    "- Main taks: remove stop words, stem/lemmatize words\n",
    "\n",
    "## 3.1 Remove stop words\n",
    "<font style=\"color:red\">English stop words file: https://juniorworld.github.io/python-workshop/doc/stop_words_eng.txt</font><br>\n",
    "Stop words are useless for understanding text.<br>\n",
    "- English: at, in, on, for, of, a, an, the...<br>\n",
    "- Chinese: 的，地，得，了.<br>\n",
    "\n",
    "However, the combination of 得了 is not a stop word.<br>\n",
    "-> Absolute Match\n",
    "\n",
    "Solution: Membership check - Check whether a word is in the predefined stopword list\n",
    ">```python\n",
    "x in [word1, word2, word3]\n",
    "x not in [word1, word2, word3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=open(\"./doc/stop_words_eng.txt\",'r')\n",
    "stopwords=[]\n",
    "for i in stopwords:\n",
    "    stopwords.append(i.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use List Comprehension technique to simplify syntaxes\n",
    "stopwords=open(\"./doc/stop_words_eng.txt\",'r')\n",
    "stopwords=[i.strip() for i in stopwords.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#go over every word in the list and check if it is a stop word\n",
    "#if not, add it to new list words_rm\n",
    "words_rm=[]\n",
    "for word in words:\n",
    "    #write your code here\n",
    "\n",
    "    \n",
    "words_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a user function called remove_stopwords()\n",
    "#Function Input: token list\n",
    "#Function Output: token list without stop words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remove_stopwords(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Stem/Lemmatize Words\n",
    "- Use external package `gensim` and `nltk` to achieve word stemming and lemmatization.\n",
    "- Words can have many variants and derivations. The goal of stemming and lemmatization is to convert the words back into their roots.\n",
    "- **Stem**: Crude. Trim off the suffixes and derivational affixes of words without knowledge of the context. Results are called stems.\n",
    "    - Advantage: Simple, Fast\n",
    "    - Disadvantage: Less accurate. Results are incomplete word roots. Cannot identify complex, context-based variants, such as comparative.\n",
    "    - Example: women -> women, apples -> appl, likely -> like, better -> better\n",
    "    >```python\n",
    "    from gensim.parsing.porter import PorterStemmer\n",
    "    Stemmer=PorterStemmer()\n",
    "    Stemmer.stem('word')\n",
    "    Stemmer.stem_documents(list_of_words)\n",
    "    ```\n",
    "- **Lemmatization**: Consider the context (part of speech of words) and converts the word to its meaningful base form, which is called Lemma.\n",
    "    - Advantage: Accurate and Contextualized. Results are meaningful, complete words.\n",
    "    - Disadvantage: Computationally Costly. Slow. Need to specify the part of speech. NLTK only supports lemmatizing nouns, adjs, and verbs.\n",
    "    - Example: women -> woman, apples -> apple, likely -> likely, is/are -> be, better -> good\n",
    "  \n",
    "    >```python\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    nltk.download('wordnet')\n",
    "    Lemmatizer=WordNetLemmatizer()\n",
    "    Lemmatizer.lemmatize('word','pos')\n",
    "    #pos = part of speech, 'n'=noun [default], 'a'=adjective, 'v'=verb, 'r'=adverbs\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context=ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "Stemmer=PorterStemmer()\n",
    "Stemmer.stem('walking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Stemmer.stem('cats'),Stemmer.stem('apples'),Stemmer.stem('women'),Stemmer.stem('likely'),Stemmer.stem('is'),Stemmer.stem('better'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stem the words_rm\n",
    "#Store results as a list named words_stem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "Lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Lemmatizer.lemmatize('cats'),Lemmatizer.lemmatize('apples'),Lemmatizer.lemmatize('women'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Lemmatizer.lemmatize('likely','a'),Lemmatizer.lemmatize('is','v'),Lemmatizer.lemmatize('better','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS is critical to results\n",
    "print(Lemmatizer.lemmatize('likely'),Lemmatizer.lemmatize('is'),Lemmatizer.lemmatize('better'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**<b>Reminder</b>** NLTK's lemmatizer is better used together with its nltk.pos_tag() function.<br>\n",
    "    The resulting tags are annotated according to: <a href=\"https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html\">https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html</a><br>\n",
    "    POS of interest: Starting with 'N','J','V'</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.pos_tag(words_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(words_rm):\n",
    "    words_lemma=[]\n",
    "    for word,pos in nltk.pos_tag(words_rm):\n",
    "        if pos[0]=='N':\n",
    "            words_lemma.append(Lemmatizer.lemmatize(word))\n",
    "        elif pos[0]=='J':\n",
    "            words_lemma.append(Lemmatizer.lemmatize(word,'a'))\n",
    "        elif pos[0]=='V':\n",
    "            words_lemma.append(Lemmatizer.lemmatize(word,'v'))\n",
    "        else:\n",
    "            words_lemma.append(word)\n",
    "    return(words_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply the above program using a dictionary\n",
    "pos_dic = {'N':'n', 'J':'a', 'V':'v'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(words_rm,pos_dic):\n",
    "    words_lemma=[]\n",
    "    for word,pos in nltk.pos_tag(words_rm):\n",
    "        \n",
    "        \n",
    "    return(words_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_lemma=lemmatization(words_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download file from: https://juniorworld.github.io/python-workshop/doc/trump_tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "table=pd.read_csv('https://juniorworld.github.io/python-workshop/doc/trump_tweets.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a program to:\n",
    "#Step 1: clean text\n",
    "#Step 2: tokenize text\n",
    "#Step 3: remove stop words from tokens\n",
    "#Step 4: lemmatize tokens. Tokens in the same tweet are saved at the same line.\n",
    "words_lemmas = []\n",
    "for text in table['tweet']:\n",
    "    #Write your code here\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "file=open('trump_twitter_tokens.csv','w',newline='\\n')\n",
    "writer=csv.writer(file)\n",
    "writer.writerows(words_lemmas)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_all = [word for doc in words_lemmas for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lemmas_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_freq=pd.Series(lemmas_al).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_freq=lemmas_freq.reset_index()\n",
    "lemmas_freq.columns=['word','freq']\n",
    "lemmas_freq[['freq','word']].to_csv('trump_wordcloud.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUIZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud\n",
    "1. HTML5 Word Cloud: http://timc.idv.tw/wordcloud/#\n",
    "2. Python package `wordcloud`\n",
    "   - WordCloud(width, height, background_color, margin, max_words, mask, random_state, stopwords)\n",
    "     - width, height: the size of the wordcloud, default = 400\n",
    "     - background_color: default = \"black\"\n",
    "     - margin: the width of space between the boundary of wordcloud and the edge of the entire image\n",
    "     - max_words: number of words displayed in the wordclouds\n",
    "     - mask: using a mask to display wordclouds in arbitrary shapes\n",
    "     - stopwords: the words that you want to exclude in the wordclouds\n",
    "     - random_state: the controller of word randomization process\n",
    "    - WordCloud.generate(text): input value should be a natural string\n",
    "    - WordCloud.generate_from_frequencies(dict): input value should be a dictionary mapping strings (words) to floats (frequencies).\n",
    ">```python\n",
    ">from wordcloud import WordCloud\n",
    ">#Initiate a WordCloud instance\n",
    ">wordcloud_generator = WordCloud(max_words=1000, margin=10)\n",
    ">\n",
    ">#Two approaches to creating wordclouds\n",
    ">#1: generate from full text (cleaned)\n",
    ">wordcloud_generator.generate(text)\n",
    ">#2: generate from word frequency table\n",
    ">wordcloud_gemerator.generate_from_frequencies(dict)\n",
    "```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud_generator = WordCloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1: Generate wordcloud based on the lemmas_freq\n",
    "# You need to Convert the dataframe (series/column) into dictionary using series.to_dict() method\n",
    "lemmas_freq.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = wordcloud_generator.generate_from_frequencies(lemmas_freq.to_dict())\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Generate wordclouds based on the cleaned text\n",
    "# You need to merge all words into a continuous string\n",
    "' '.join(lemmas_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_text = ' '.join(lemmas_all)\n",
    "wordcloud = wordcloud_generator.generate(joined_text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .join() method can also be applied to a column of strings\n",
    "' '.join(table['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate wordclouds based on raw text\n",
    "# Request the WordCloud to remove stopwords for you\n",
    "# The absence of first-run data cleaning will lead to the inclusion of irrelevant or nonsensical characters in the word cloud.\n",
    "text = ' '.join(table['tweet'])\n",
    "wordcloud_generator = WordCloud(stopwords = stopwords)\n",
    "wordcloud = wordcloud_generator.generate(text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Co-occurence Network\n",
    "Download Gephi from: https://gephi.org/users/download/ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract mentions from his tweets\n",
    "# Generate a Co-Mention Network\n",
    "# Where nodes represent twitter handlers and links represent co-mention relationship in tweets\n",
    "# Hint: \n",
    "# Step 1: Check if the post has more than one mentions\n",
    "# Step 2: If yes, save mentions within a post into a row\n",
    "file=open('trump_twitter_comentions.csv','w',newline='\\n')\n",
    "writer=csv.writer(file)\n",
    "for text in table['tweet']:\n",
    "    #Write your code here\n",
    "    \n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Vectorization: Word Embeddings\n",
    "- Word embedding is a technique to transform textual words into a numerical representation (word vector). Each word is mapped to one vector and this vector is trained to learn the syntactic and semantic relationships between words.\n",
    "    - e.g.: \"book\" (1,0,3), \"paper\" (1,0,4) -> synonyms are close in the word space\n",
    "- Applications:\n",
    "    - Find out synonyms in the corpus (a list of documents). A way to understand the structure of opinion expression.\n",
    "        - `word_embedding.wv.most_similar(word, n=n)` find out the most similar n word for a focal word\n",
    "        - In Trump's opinions, CNN is a byword for enemy and dishonesty, while Clinton is a synonym for crook.\n",
    "    - Find out the equivalent mapping between words and concepts. It can understand the meaning of word combination by assuming the meaning of words is transmissive.\n",
    "        - `word_embedding.wv.most_similar(positive=[word1, word2], negative=[word3])`\n",
    "        - \"queen\" is to \"women\" is what \"king\" is to \"men\"\n",
    "        - \"Beijing\" is to \"China\" is what \"Tokyo\" is to \"Japan\"\n",
    "        - Vectors are eligible for math operations, like + and -\n",
    "    - Find out the least similar (least possible) word in a sentence\n",
    "        - `word_embedding.wv.doesnt_match([word1, word2 ...])`\n",
    "    - Evaluate the similarity (reverse of distance) between two or more sentences\n",
    "    - Evaluate the possibility of a sentence belonging to this corpus\n",
    "        - `word_embedding.score([word1, word2 ...])`\n",
    "\n",
    "<img src=\"https://juniorworld.github.io/python-workshop/img/word2vec_2.png\" width=\"300\">\n",
    "<img src=\"https://juniorworld.github.io/python-workshop/img/word2vec.png\" width=\"500\">\n",
    "Reference: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('trump_twitter_tokens.csv','r')\n",
    "reader = csv.reader(file)\n",
    "word_doc_matrix=[]\n",
    "for row in reader:\n",
    "    word_doc_matrix.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word document matrix\n",
    "#rows: documents\n",
    "#columns: words\n",
    "word_doc_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install gensim==3.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections.abc\n",
    "#hyper needs the four following aliases to be done manually.\n",
    "collections.Iterable = collections.abc.Iterable\n",
    "collections.Mapping = collections.abc.Mapping\n",
    "collections.MutableSet = collections.abc.MutableSet\n",
    "collections.MutableMapping = collections.abc.MutableMapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a small corpus. We set the vector dimension to be as small as 15 only.\n",
    "# Word embedding trained on rich corpus like Wikipedia usually will take a dimensionality at 200 or more.\n",
    "word_embedding = Word2Vec(sentences=words_lemmas, size=30, window=5, min_count=1, workers=4, hs=1, negative=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10 synonyms most similar to a target word in the word space\n",
    "# Try: cnn, clinton, biden, election, china\n",
    "word_embedding.wv.most_similar('cnn', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get synonyms based on a target vector \n",
    "# Xi is to China as XX is to America\n",
    "word_embedding.wv.most_similar(positive=['china','america'], negative=['xi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the possibility that Trump says such a sentence in his Twitter account\n",
    "# The higher score, the more probable that Trump will say this according to his past utterances.\n",
    "word_embedding.score(['cnn is friend'.split(),\n",
    "                      'cnn is enemy'.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding.score(['china is friend'.split(),\n",
    "                      'china is enemy'.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Predict rating of a review comment of Amazon Alexa\n",
    "1. Open this data frame: https://juniorworld.github.io/python-workshop/doc/amazon_alexa.tsv\n",
    "2. Train two Word2Vec models of word representations, for 5-star reviews and non-5-star reviews respectively\n",
    "3. Predict the rating of a comment, saying \"It doesn't answer me sometimes and give me some irrelevant answers quite often\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexa_reviews=pd.read_csv('https://juniorworld.github.io/python-workshop/doc/amazon_alexa.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexa_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many 5-star, 4-star, 3-star, 2-star, 1-star ratings?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_stars_comments=[]\n",
    "for text in :\n",
    "    text=data_cleaning(text)\n",
    "    #write your code here\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_five_stars_comments=[]\n",
    "for text in :\n",
    "    text=data_cleaning(text)\n",
    "    #write your code here\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star_model=Word2Vec(sentences=five_stars_comments, vector_size=30, window=5, min_count=1, workers=4, hs=1, negative=0)\n",
    "non_five_star_model=Word2Vec(sentences=non_five_stars_comments, vector_size=30, window=5, min_count=1, workers=4, hs=1, negative=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"It doesn't answer me sometimes and give me some irrelevant answers quite often.\"\n",
    "test_clean = data_cleaning(test_sentence)\n",
    "test_words = test_clean.split()\n",
    "test_rm = remove_stopwords(test_words)\n",
    "test_lemma = lemmatization(test_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star_model.score([test_lemma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_five_star_model.score([test_lemma])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
