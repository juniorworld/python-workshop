{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class12 - NLP - CHN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://juniorworld.github.io/python-workshop/img/NLP_.png\" width=\"600px\" height=\"400px\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "- Word embedding is a technique to transform textual words into a numerical representation (word vector). Each word is mapped to one vector and this vector is trained to learn the syntactic and semantic relationships between words.\n",
    "    - e.g.: \"book\" (1,0,3), \"paper\" (1,0,4) -> synonyms are close in the word space\n",
    "- Applications:\n",
    "    - Find out synonyms in the corpus (a list of documents). A way to understand the structure of opinion expression.\n",
    "        - In Trump's opinions, CNN is a byword for enemy and dishonesty, while Clinton is a synonym for crook.\n",
    "    - Find out the equivalent mapping between words and concepts. It can understand the meaning of word combination by assuming the meaning of words is transmissive.\n",
    "        - \"queen\" to \"women\" is what \"king\" to \"men\"\n",
    "        - \"Beijing\" to \"China\" is what \"Tokyo\" to \"Japan\"\n",
    "        - Vectors are eligible for math operations, like + and -\n",
    "    - Find out the least similar (least possible) word in a sentence\n",
    "    - Evaluate the similarity (distance) between two or more sentences\n",
    "    - Evaluate the possibility of a sentence belonging to this corpus\n",
    "        - Corpus: A set of text documents\n",
    "\n",
    "<img src=\"https://juniorworld.github.io/python-workshop/img/word2vec.png\" width=\"400\">\n",
    "Reference: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the trump_twitter_tokens.csv that we created last week\n",
    "# If you couldn't find this file in your computer, please download it from: https://juniorworld.github.io/python-workshop/doc/trump_twitter_tokens.csv\n",
    "import csv\n",
    "file=open('trump_twitter_tokens.csv','r')\n",
    "reader = csv.reader(file)\n",
    "tokens=[]\n",
    "for row in reader:\n",
    "    tokens.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word document matrix\n",
    "#rows: documents\n",
    "#elements: words\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=1, workers=4, hs=1, negative=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10 synonyms most similar to a target word in the word space\n",
    "# Try: cnn, clinton, biden, election, china\n",
    "model.wv.most_similar('cnn', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get synonyms based on a target vector \n",
    "# Tariff is to china is what XX is to usa\n",
    "model.wv.most_similar(positive=['china','usa'], negative=['tariff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the least similar word in a list of words\n",
    "model.wv.doesnt_match(['china','usa','canada','cnn','mexico'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the possibility that Trump says such a sentence in his Twitter account\n",
    "model.score(['cnn is friend'.split(),\n",
    "             'cnn is enemy'.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(['china is friend'.split(),\n",
    "             'china is enemy'.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Load the user functions that we defined last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "Lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub('[0-9]+','',text)\n",
    "    text=re.sub('@[^ ]+','',text)\n",
    "    text=re.sub('#[^ ]+','',text)\n",
    "    text=re.sub('https:[^ ]+','',text)\n",
    "    text=re.sub('[\\W]+',' ',text)\n",
    "    text=text.strip()\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_eng=open(\"stop_words_eng.txt\",'r')\n",
    "stopwords_eng=[i.strip() for i in stopwords.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_eng(words):\n",
    "    words_rm=[]\n",
    "    for word in words:\n",
    "        if word not in stopwords_eng:\n",
    "            words_rm.append(word)\n",
    "    return(words_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(words_rm):\n",
    "    words_lemma=[]\n",
    "    for word,pos in nltk.pos_tag(words_rm):\n",
    "        if pos[0]=='N':\n",
    "            words_lemma.append(Lemmatizer.lemmatize(word))\n",
    "        elif pos[0]=='J':\n",
    "            words_lemma.append(Lemmatizer.lemmatize(word,'a'))\n",
    "        elif pos[0]=='V':\n",
    "            words_lemma.append(Lemmatizer.lemmatize(word,'v'))\n",
    "        else:\n",
    "            words_lemma.append(word)\n",
    "    return(words_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict rating of a review comment of Amazon Alexa\n",
    "1. Open this data frame: https://juniorworld.github.io/python-workshop/doc/amazon_alexa.tsv\n",
    "2. Train two Word2Vec models of word representations, for 5-star reviews and non-5-star reviews respectively\n",
    "3. Predict the rating of a comment, saying \"It doesn't answer me sometimes and give me some irrelevant answers quite often\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexa_reviews=pd.read_csv('https://juniorworld.github.io/python-workshop/doc/amazon_alexa.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexa_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many 5-star, 4-star, 3-star, 2-star, 1-star ratings?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_stars_comments=[]\n",
    "for text in : #fill in your code\n",
    "    text=data_cleaning(text)\n",
    "    #write your code here\n",
    "    \n",
    "    \n",
    "    five_stars_comments.append(words_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_five_stars_comments=[]\n",
    "for text in : #fill in your code\n",
    "    text=data_cleaning(text)\n",
    "    #write your code here\n",
    "    \n",
    "    non_five_stars_comments.append(words_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star_model=Word2Vec(sentences=five_stars_comments, vector_size=100, window=5, min_count=1, workers=4, hs=1, negative=0)\n",
    "non_five_star_model=Word2Vec(sentences=non_five_stars_comments, vector_size=100, window=5, min_count=1, workers=4, hs=1, negative=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_star_model.score([data_cleaning(\"It doesn't answer me sometimes and give me some irrelevant answers quite often\").split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_five_star_model.score([data_cleaning(\"It doesn't answer me sometimes and give me some irrelevant answers quite often\").split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Special about Chinese Language?\n",
    "1. First-run Cleaning: No need to convert <font style=\"color: blue\">letter case</font>\n",
    "    - data_cleaning() solely for chinese language will be slighter than that for english, without a line of text.lower()\n",
    "    - However, we typically will use english-version data_cleaning(), in case there are some english character in the text\n",
    "2. Tokenization: No <font style=\"color: blue\">natural deliminator</font>, like the space in Eng. Need to rely on language model to split text into words.\n",
    "3. Second-run Cleanig: No need to <font style=\"color: blue\">stem/lemmatize</font> words\n",
    "4. Vectorization: Identical to English\n",
    "\n",
    "#### 1. First-run Data Cleaning\n",
    "- Main task: Remove punctuations and special characters like hashtags, hyperlinks\n",
    "- Use Regular Expression for Pattern Matching\n",
    "- No need to convert cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also works for Chinese text\n",
    "re.sub('[\\W]+',' ','普京表示，歡迎中方在化解危機中的建設性角色！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the data_cleaning() function with a Weibo post\n",
    "a=\"各國應轟炸俄羅斯境內“暈輸線”……當年“炮擊金門”很久，最後因美國切斷了“廈門車站”運輸線，炮擊金門才止。（而不應去烏克蘭建軍工廠：俄會集中火力轟炸。）@美国驻华大使馆 @英國駐華使館 @歐盟在中國 @烏克蘭信使\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tokenization\n",
    "- Definition: tokenization is a process of splitting sentences/paragraphs/documents into a set of words.\n",
    "- Differences in Languages:\n",
    "    - English: **words** are naturally separated with spaces\n",
    "    - Korean: **phrases** are naturally separated with spaces\n",
    "        - konlpy (http://konlpy.org/)\n",
    "    - Chinese/Japanese: **no spaces** in text\n",
    "        - Chinese: jieba (https://github.com/fxsjy/jieba)\n",
    "        - Japanese: jNlp (https://github.com/kevincobain2000/jProcessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a package package \"jieba\" to tokenize Chinese text.<br>\n",
    "<br>\n",
    "**Why jieba?**\n",
    "- It adopts a hybrid method combining both statistical/probabilistic inference and pattern matching based on dictionary. \n",
    "    - capable to recognize words existing in the pre-defined dictionary\n",
    "    - capable to find new words.\n",
    "- Two dictionaries:\n",
    "    - System dictionary\n",
    "        - Simplied Chinese\n",
    "        - Simplied+Traditional Chinese\n",
    "    - User dictionary\n",
    "- Syntax:\n",
    ">```python\n",
    "jieba.cut(sentence) #result is a list of words\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(jieba.cut('你好，这是一个简单的句子。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it can segment tradional Chinese text by using statistical inference method.\n",
    "list(jieba.cut('你好，這是一個簡單的句子。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#however, statistical inference is not always perfect.\n",
    "list(jieba.cut('談判擱置，工會號召靜坐。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(jieba.cut('谈判搁置，工会号召静坐。'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we improve statistical inference for the tokenization?<br>\n",
    "**Human in the loop**: Provide human-defined dictionary to constrain and fine-tune the statistical inference.\n",
    "##### Solution: Configurate Dictionaries\n",
    "- Two types of dictionaries:\n",
    "    1. System dictionary: General purpose\n",
    "    2. User dictionary: Special context, e.g. dictionaries for emotion, incivility, war\n",
    "- How does the dictionary look like?\n",
    "    - Don't confuse with the data type “dictionary”\n",
    "    - Dictionary is a plain text file\n",
    "    - One line one keyword, similar to the stopword list/file\n",
    "    - [Optional] Words might also be weighted, carrying with a number/decimal suggestive of the importance of the words\n",
    "\n",
    ">```python\n",
    "#Way 1: no weight: all words are created equal\n",
    "China\n",
    "People's Republic of China\n",
    "China Central Television\n",
    "\n",
    ">```python\n",
    "#Way 2: with weights: words are treated unequally. Higher weight, Higher priority\n",
    "China,3\n",
    "People's Republic of China, 4\n",
    "China Central Television,4\n",
    "\n",
    "\n",
    "- In jieba, you can load dictionaries using the following syntaxes:\n",
    ">```python\n",
    "jieba.set_dictionary(\"path_of_system_dict\") \n",
    "jieba.load_userdict(\"path_of_user_dict\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better segment traditional Chinese text, we need to upgrade system dictionary to include traditional Chinese words.<br>\n",
    "Download the system dictionary from this link:https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load traditional Chinese system dictionary\n",
    "jieba.set_dictionary('dict.txt.big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try tokenizing this sentence again\n",
    "list(jieba.cut('談判擱置，工會號召靜坐。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some names and special terminologies cannot be properly identified.\n",
    "print(list(jieba.cut('中央上周二向特首林鄭月娥發公函'))) #very long name\n",
    "print(list(jieba.cut('蔡英文日前表示希望與日本舉行安保對話'))) #names including frequently used words\n",
    "print(list(jieba.cut('高雄市長韓國瑜本月稍後訪問港澳深圳廈門四市'))) #names including frequently used words\n",
    "print(list(jieba.cut('汶萊的全稱為汶萊達魯薩蘭國。'))) #special terminologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a for loop to build your user dictionary (time-consuming)\n",
    "file=open('user_dict.txt','w',encoding='utf-8')\n",
    "keywords=['林鄭月娥','蔡英文','韓國瑜','汶萊達魯薩蘭國']\n",
    "#Write your loop here\n",
    "\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use your user dictionary\n",
    "jieba.load_userdict('user_dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After loading user dictionary:\n",
    "print(list(jieba.cut('中央上周二向特首林鄭月娥發公函'))) #very long name\n",
    "print(list(jieba.cut('蔡英文日前表示希望與日本舉行安保對話'))) #names including frequently used words\n",
    "print(list(jieba.cut('高雄市長韓國瑜本月稍後訪問港澳深圳廈門四市'))) #names including frequently used words\n",
    "print(list(jieba.cut('汶萊的全稱為汶萊達魯薩蘭國。'))) #terminologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove stop words\n",
    "<font style=\"color:red\">English stop words file: https://juniorworld.github.io/python-workshop/doc/stop_words_eng.txt</font><br>\n",
    "Stop words are useless for understanding text.<br>\n",
    "- English: at, in, on, for, of, a, an, the...<br>\n",
    "- Chinese: 的，地，得，了.<br>\n",
    "- Solution: Membership check - Check whether a word is in the predefined stopword list\n",
    ">```python\n",
    "x in [word1, word2, word3]\n",
    "x not in [word1, word2, word3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chinese stop words file: https://juniorworld.github.io/python-workshop/doc/stop_words_chi.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load stop word list\n",
    "file_chi=open('stop_words_chi.txt','r',encoding='utf-8')\n",
    "stop_words_chi=[i.strip() for i in file_chi.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(stop_words_chi) #much longer and detailed than english stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#have a look at the dictionary\n",
    "stop_words_chi[34:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph='Facebook CEO 馬克·朱克伯格（Mark Zuckerberg）週三發布了一篇長文，闡述了要將 Facebook 打造成「以隱私為中心的平台」的願景，並表示將打通 Messenger、Instagram 和 WhatsApp 用戶之間的交流阻礙。朱克伯格表示，他相信未來人們的溝通行為會更多轉向私人加密服務，也未必希望他們分享的所有內容都被永遠保存在互聯網上——後者對於每個人來說，既可能是財富，也可能是負擔。因此，儘管 Facebook 長期以來專注於打造開放、分享的社區平台，但他認為，以隱私為中心的通信平台會比當今的開放平台更加重要。'\n",
    "# Step 1: Remove punctuation\n",
    "# Step 2: Tokenize the sentence\n",
    "# Step 3: Remove stop words\n",
    "# Save the token list as words\n",
    "#------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count word frequency\n",
    "pd.Series(words).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a user function called remove_stopwords_chi()<br>\n",
    "For your reference, here is the user function for removing stopwords in English text:\n",
    ">```python\n",
    "def remove_stopwords_eng(words):\n",
    "    words_rm=[]\n",
    "    for word in words:\n",
    "        if word not in stopwords_eng:\n",
    "            words_rm.append(word)\n",
    "    return(words_rm)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a generalized function for stopword removal, which can be applied for any language\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:blue'>Practice</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the 10 fade-in and fade-out words in speeches.<br>\n",
    "The magnitude of difference is measured by the change in their relative frequencies:<br>\n",
    "<p style='text-align:center;font-size:15px;'>Relative Freq (RF) = word frequency / sum of word frequencies</p>\n",
    "<p style='text-align:center;font-size:15px;'>Difference = RF<font size='2px'>2019</font> - RF<font size='2px'>2009</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options:<br>\n",
    "- Chinese: Annual government work reports, <a href=\"https://juniorworld.github.io/python-workshop/doc/2019_Government_Work_Report.txt\">2019</a> vs <a href=\"https://juniorworld.github.io/python-workshop/doc/2009_Government_Work_Report.txt\">2009</a>\n",
    "- English: State of the Union address, <a href=\"https://juniorworld.github.io/python-workshop/doc/2019_SoU.txt\">2019</a> vs <a href=\"https://juniorworld.github.io/python-workshop/doc/2009_SoU.txt\">2009</a><br>\n",
    "\n",
    "*Hint:*<br>\n",
    "*1. Use `pd.concat([df1,df2],axis=1)` to combine two dataframes by columns and `pd.concat([df1,df2],axis=0)` to combine two dataframes by rows*<br>\n",
    "*2. Use `df[column_name].value_counts()` to count the items in a column.*<br>\n",
    "*3. Use `df.sort_values(column_name,ascending=True)` to sort a certain column. To get a reversed list, you can set ascending=False* <br>\n",
    "*4. Use `df.fillna(0)` to replace NAN value with 0.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq_words=pd.Series(words).value_counts()\n",
    "freq_words/max(freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Chinese files\n",
    "Chi_file_2019=open('2019_Government_Work_Report.txt','r',encoding='utf-8')\n",
    "Chi_file_2009=open('2009_Government_Work_Report.txt','r',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHI 2019\n",
    "#Step1: Clean text\n",
    "#Step2: Tokenize text\n",
    "#Step3: Remove stopwords\n",
    "#Step4: Add the current word list to Chi_words_2019\n",
    "#---------------------------------------------------\n",
    "Chi_words_2019=[]\n",
    "for line in file_2019.readlines(): #each line is a paragraph\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHI 2009\n",
    "Chi_words_2019=[]\n",
    "for line in file_2019.readlines():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count word frequencies\n",
    "Chi_freq_2019=\n",
    "Chi_freq_2009="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count relative frequency\n",
    "#Hint: You can use sum() function or .sum() method to get the total of a numeric column\n",
    "relative_freq_2019=\n",
    "relative_freq_2009="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine relative_freq_2019 and relative_freq_2009, using pd.concat() function\n",
    "relative_freq="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill missing values\n",
    "relative_freq=relative_freq.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the frequency difference\n",
    "relative_freq['diff']="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change column names\n",
    "relative_freq.columns=['2019','2009','diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort table by column 'diff'\n",
    "#Fade in words: words that are more common in 2019 report\n",
    "relative_freq.sort_values(your_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fade out words: words that are more common in 2009 report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read English files\n",
    "Eng_file_2019=open('2019_SoU.txt','r',encoding='utf-8')\n",
    "Eng_file_2009=open('2009_SoU.txt','r',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
