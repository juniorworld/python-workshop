{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://juniorworld.github.io/python-workshop-2018/img/portfolio/week10.jpg\" width=\"350px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train a model to give predictions based on **labeled data** (X + Y)\n",
    "- Information Retrieval: KNN\n",
    "- Regression: \n",
    "    - Linear regression\n",
    "    - Generalize Linear Model: logistic (binary), poisson (count)\n",
    "- Classification:\n",
    "    - Binary Classification: Naive Bayesian classifier\n",
    "    - Multiclass Classification: Multinomial Bayesian classifier, KNN\n",
    "    - (Advanced) Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedure:\n",
    "- STEP 1: Split dataset\n",
    "    - 2 parts: train vs test, e.g. 60:40 or 70:30\n",
    "    - 3 parts: train vs test vs validation: e.g. 60:30:10 or 70:20:10 [not usual]\n",
    "- STEP 2: Train the model\n",
    "- STEP 3: Test the model\n",
    "- STEP 4: Parameter tuning. If the result is not satisfactory, retrain the model with new parameters and retest the newly trained model.\n",
    "- STEP 5: Report the model performance with Test/Validation data.\n",
    "- STEP 6: Apply the model to new data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "- Distance-based Spatial Voting Model\n",
    "- Purpose: Retrieve the most similar information from database + Classify a new point according its nearest neighbors\n",
    "- Input: a set of data with labels\n",
    "- Output: K nearest neighbors\n",
    "    - Assign the category according K's labels\n",
    "- Parameter: K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Movie|#Fight scenes|#Kiss scenes|Genre|\n",
    "|-----|:-----------:|:----------:|:---:|\n",
    "|California Man|3|104|Love|\n",
    "|He's not that into you|2|100|Love|\n",
    "|Beautiful Woman|1|81|Love|\n",
    "|Kevin Longblade|101|10|Action|\n",
    "|Robo Slayer 3000|99|5|Action|\n",
    "|Amped II|98|2|Action|\n",
    "|<font style='color:blue'>XXXXX</font>|<font style='color:blue'>18</font>|<font style='color:blue'>90</font>|<font style='color:red'>?</font>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import *\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index=np.random.choice(range(150),100,replace=False)\n",
    "train_X=iris.data[train_index]      #extract 100 data records as our training data\n",
    "train_Y=iris.target[train_index]    #training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index=[i for i in range(150) if i not in train_index]\n",
    "test_X=iris.data[test_index]\n",
    "test_Y=iris.target[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.shape)\n",
    "print(test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0] #first data point in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance between first test data and first train data\n",
    "distance="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distances between first test data and ALL train data\n",
    "distances=[]\n",
    "for i in train_X:\n",
    "    distance=\n",
    "    distances.append(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suppose K=4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(distances)[:4] #extract the indexes of K smallest distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the labels of those points\n",
    "KNNs=train_Y[np.argsort(distances)[:4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,3,4,5,5,4,4,4,4,2,1]\n",
    "pd.Series(a).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the most frequent label and use it as your predicted label for first testing point\n",
    "pd.Series(KNNs).value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a for loop to go over every testing data and predict their labels\n",
    "predict_Y=[]\n",
    "for j in test_X:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance diagnosis\n",
    "- **Accuracy** rate: \n",
    "    - formula: true predictions/total sample size\n",
    "- **Precision** rate:\n",
    "    - formula: true positive/predicted positive\n",
    "    - **macro**: calculate the precision of each label and get their means\n",
    "    - micro: sum up the number of true positive and get the total precision [= accuracy]\n",
    "- **Recall** rate:\n",
    "    - formula: true positive/real positive\n",
    "    - **macro**\n",
    "    - micro\n",
    "- **F1** score\n",
    "<img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/057ffc6b4fa80dc1c0e1f2f1f6b598c38cdd7c23'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis=pd.crosstab(predict_Y,test_Y, rownames=['predict'], colnames=['real'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diagonal(diagnosis) #extract numbers on diagnonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric1: ACC\n",
    "accuracy_rate=sum(np.diagonal(diagnosis))/len(predict_Y)\n",
    "accuracy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric2: precision\n",
    "precisions=np.diagonal(diagnosis)/np.sum(diagnosis,axis=1) #diagnoals in row sums\n",
    "precision_rate=np.mean(precisions)\n",
    "precision_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric3: recall\n",
    "recalls=np.diagonal(diagnosis)/np.sum(diagnosis,axis=0) #diagnoals in col sums\n",
    "recall_rate=np.mean(recalls)\n",
    "recall_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metric4: F1\n",
    "f1s=2*precisions*recalls/(precisions+recalls)\n",
    "F1=np.mean(f1s)\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(test_Y, predict_Y))\n",
    "print(precision_score(test_Y, predict_Y, average='macro'))\n",
    "print(recall_score(test_Y, predict_Y, average='macro'))\n",
    "print(f1_score(test_Y, predict_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "Set K=6. Please apply KNN technique to digit data:\n",
    "1. Split the data into training set (1000 samples) and testing set (797 samples).\n",
    "2. Apply KNN\n",
    "3. Report model performance metrics (accuracy, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning\n",
    "- Purpose: find the best parameter\n",
    "- For KNN, the only parameter is K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap previous lines into a function\n",
    "def KNN(K,train_X,train_Y,train_X,test_Y):\n",
    "    predict_Y=[]\n",
    "    \n",
    "    #WRITE YOUR CODE HERE\n",
    "    \n",
    "    predict_Y=np.array(predict_Y)\n",
    "    return(predict_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s=[]\n",
    "for K in range(1,10):\n",
    "    predict_Y=KNN(K,train_X,train_Y,test_X,test_Y)\n",
    "    f1s.append(f1_score(test_Y, predict_Y, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Break\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://cdn-images-1.medium.com/max/900/1*XMId5sJqPtm8-RIwVVz2tg.png' width='300px' align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Tree is a non-parametric supervised learning method.\n",
    "- [Advanced] Random Forest\n",
    "- STEP 1: Calculate the entropy of labels\n",
    "- STEP 2: Choose a variable and split the dataset along that variable\n",
    "- STEP 3: Calculate the entropy of splitting\n",
    "- STEP 4: Repeat STEP 2-3 for all variables\n",
    "- STEP 5: Find the variable with greatest information gain\n",
    "- STEP 6: Add it to the root\n",
    "- STEP 7: Repeat STEP 2-6 for all variables\n",
    "- STEP 8: Build up the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "- Entropy: Information load\n",
    "    - formula: <img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/7de5d59a442f5305853d4392826b1f51dc43f6d0' width='200px'>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data):\n",
    "    freq=pd.Series(data).value_counts()\n",
    "    freq=freq/sum(freq)\n",
    "    H=sum(-freq*np.log2(freq))\n",
    "    return(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[2,2,2,2,2,2,2,2] #one group\n",
    "b=[1,1,1,1,2,2,2,2] #two balanced group\n",
    "c=[1,2,2,2,2,2,2,2] #two unbalanced group\n",
    "Ha=entropy(a)\n",
    "Hb=entropy(b)\n",
    "Hc=entropy(c)\n",
    "print(Ha)\n",
    "print(Hb)\n",
    "print(Hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to what degree one variable can inform us about another variable\n",
    "var1=['red','blue','red','blue']\n",
    "var2=[0,1,1,1]\n",
    "df=pd.DataFrame({'color':var1,'number':var2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into two parts and check the entropy of 'number'\n",
    "H1=entropy(df[df['color']=='red']['number'])\n",
    "H1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H2=entropy(df[df['color']=='blue']['number']) #exteremly certain\n",
    "H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H=H1+H2 #integrated entropy for two groups\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df=pd.DataFrame(iris.data,columns=iris.feature_names)\n",
    "iris_df['label']=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use threshold of sepal length at 5.0 first and find out the entropy of this splitting\n",
    "threshold=5\n",
    "H1=entropy(iris_df[iris_df['sepal length (cm)']>=threshold]['label'])\n",
    "H2=entropy(iris_df[iris_df['sepal length (cm)']<threshold]['label'])\n",
    "H1+H2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.iloc[:,0] #reference by column index, rather than column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find out the threshold that can produce minimal entropy\n",
    "def best_threshold(data,col): #col is the column index that you want to apply threshold searching\n",
    "    thresholds=np.unique(data.iloc[:,col]) #threshold candidates\n",
    "    Hs=[]\n",
    "    for threshold in thresholds:\n",
    "        \n",
    "        #WRITE YOUR CODE HERE\n",
    "    \n",
    "    best_threshold=                 #best threshold is the one with minimal entropy\n",
    "    min_Hs=min(Hs)\n",
    "    return(best_threshold,min_Hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run over all columns and find out the best feature with smallest entropy\n",
    "Hs=[]\n",
    "thresholds=[]\n",
    "for i in range(4):\n",
    "    threshold,H=best_threshold(iris_df,i)\n",
    "    thresholds.append(threshold)\n",
    "    Hs.append(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(Hs) #the root feature is petal length as it has the smallest entropy -> it can inform us about labels at most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_feature(data):\n",
    "    Hs=[]\n",
    "    thresholds=[]\n",
    "    for i in range(4):\n",
    "        threshold,H=best_threshold(data,i)\n",
    "        thresholds.append(threshold)\n",
    "        Hs.append(H)\n",
    "    return(np.argmin(Hs),thresholds[np.argmin(Hs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build up Decision Tree\n",
    "- RULE: Assign the feature according to their entropy in ascending order\n",
    "- Create a dictionary about Tree:\n",
    "    - Four elements\n",
    "    - 'col' is the column index used at current level\n",
    "    - 'threshold' is its threshold\n",
    "    - 'greater' contains a dictionary about its child leaf for data points greater than the threshold\n",
    "    - 'smaller' contains a dictionary about its child leaf for data points smaller than the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">```python\n",
    "{'col':2,\n",
    " 'threshold':thresholds[2],\n",
    " 'larger':{ #containing a dictionary for child leaf where data points greater than threshold will go\n",
    "     'col': ..., \n",
    "     'threshold': ...,\n",
    "     'larger': ...,\n",
    "     'smaller': ...\n",
    " }, \n",
    " 'smaller':{ #containing a dictionary for child leaf where data points smaller than threshold will go\n",
    "     'col': ..., \n",
    "     'threshold': ...,\n",
    "     'larger': ...,\n",
    "     'smaller': ...\n",
    " }  \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To add new leaf, we need to run another run of \n",
    "def create_tree(data,max_group_size):\n",
    "    if len(np.unique(data['label']))>1 and data.shape[0]>max_group_size:\n",
    "        feature,threshold=best_feature(data)\n",
    "        feature_name=data.columns[feature]\n",
    "        tree_dict={'col':feature,'threshold':threshold}\n",
    "        #split dataset into two parts: larger and smaller than threshold\n",
    "        #this is the larger part\n",
    "        subset1=data[data[feature_name]>=threshold]\n",
    "        if subset1.shape[0] == data.shape[0]: #check whether dataset is really split into two part\n",
    "            return(data['label'].value_counts().index[0])\n",
    "        tree_dict['larger']=create_tree(subset1,max_group_size)\n",
    "        #this is the smaller part\n",
    "        subset2=data[data[feature_name]<threshold]\n",
    "        if subset2.shape[0] == data.shape[0]:\n",
    "            return(data['label'].value_counts().index[0])\n",
    "        tree_dict['smaller']=create_tree(subset2,max_group_size)\n",
    "        return(tree_dict)\n",
    "    else:\n",
    "        return(data['label'].value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iris=iris_df.sample(n=100)\n",
    "test_iris=iris_df.sample(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict=create_tree(train_iris,50)\n",
    "tree_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dict=create_tree(train_iris,30)\n",
    "tree_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tree(point,tree,predicts):\n",
    "    if point[tree['col']]>=tree['threshold']:\n",
    "        if np.isscalar(tree['larger']):\n",
    "            predicts.append(tree['larger'])\n",
    "        else:\n",
    "            apply_tree(point,tree['larger'],predicts)\n",
    "    else:\n",
    "        if np.isscalar(tree['smaller']):\n",
    "            predicts.append(tree['smaller'])\n",
    "        else:\n",
    "            apply_tree(point,tree['smaller'],predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts=[]\n",
    "for i in range(test_iris.shape[0]):\n",
    "    apply_tree(test_iris.iloc[i],tree_dict,predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(predicts,test_iris['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
