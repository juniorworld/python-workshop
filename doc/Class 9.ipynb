{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 9 - Natural Lanugage Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flowchart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://juniorworld.github.io/python-workshop/img/NLP_.png\" width=\"600px\" height=\"400px\" align='left'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demonstrate how to go through these five steps for English and Chinese texts respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. First-run Data Cleaning\n",
    "- Main task: convert the case, remove punctuations and special characters like hashtags, hyperlinks\n",
    "- Use Regular Expression for Pattern Matching\n",
    "- Convert the case: `.lower()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expression Cheat Sheet\n",
    "1. `.`: Wildcard, any character\n",
    "2. `[abc]`: Group (a or b or c)\n",
    "    - \\\\: escape following special characters\n",
    "    - special characters that need to be escaped: `^ [ . $ { * ( \\ + ) | ? < >`\n",
    "3. `[^abc]`: Reverse group, Not (a or b or c)\n",
    "4. Character class:\n",
    "    - `\\w`: Word (incl. chars and numbers), `\\W`: Non-word\n",
    "    - `\\c`: Control character, such as line break, tab\n",
    "    - `\\s`: White space, `\\S`: Not white space\n",
    "    - `\\d`: Digit = [0-9], `\\D`: Not digit = [^0-9]\n",
    "    - `[:alnum:]`: Digits and letters = [0-9a-zA-Z]\n",
    "\n",
    "5. Quantifier:\n",
    "    - `*` at least 0 times.\n",
    "    - `+` at least 1 times.\n",
    "    - `?` at most 1 times.\n",
    "    - `{n}`: Exactly n times. \n",
    "    - `{n,}`: At least n times\n",
    "    - `{n,m}`: n-m times (m>n)\n",
    "\n",
    "6. Location:\n",
    "   - `^` the start of the string\n",
    "   - `$` the end of the string\n",
    "     \n",
    "8. Logic operation: \n",
    "   - `|` or\n",
    "   - `&` and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Substrings\n",
    "- `re.sub(pattern, new_string, original_string)`\n",
    "\n",
    "### Find All Substrings\n",
    "- `re.findall(re_pattern,string)`\n",
    "- `()` Limited extraction. Specify the particular substrings that you want to extract.\n",
    "- result is a **LIST** of match substrings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet='@JerryNadler admits on #CNN they have no proof of Obstruction by @realDonaldTrump its just his \"personal opinion\" Meet the new #WitchHunt Same as the old #WitchHunt cc @DonaldJTrumpJr'\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all hashtags\n",
    "re.sub('#[^ ]+','',tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all hashtags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all mentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many people does this tweet mention?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove hyperlinks\n",
    "tweet2='Read this article: https://goo.gl/rwGHTP Tesla’s abrupt shift to online-only car sales, after racing to open stores, battered its share price and raised questions about its future.'\n",
    "tweet3=re.sub('https:[^ ]+','',tweet2)\n",
    "tweet3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all hyperlinks\n",
    "re.findall('http[^ ]+',tweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove non-word punctuations\n",
    "re.sub('[\\W]+',' ',tweet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also works for Chinese text\n",
    "re.sub('[\\W]+',' ','普京表示，歡迎中方在化解危機中的建設性角色！')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**<b>Reminder</b>** By removing punctuations, you will also loose the full stops and line breaks. You won't be able to analyze sentence and paragraph structure. So, [\\W]+ should ONLY BE USED WHEN the paragraph/sentence structure is not important for you.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove spaces and line breaks at the beginning or the end of the sentence\n",
    "text=' 普京表示\\n歡迎中方在化解危機中的建設性角色 '\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Extract all capitalized words, such as SAT, GRE, and PPT.\n",
    "2. Extract all words that are not the first words in the sentences but start with a uppercase letter.\n",
    "3. Extract all all words starting with a/A and ending with s/S.\n",
    "4. Extract all words that has an _a_ followed by one or more _s_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Across the world, it’s still unclear when Ukrainian pilots will begin training at the center, at the Fetesti air base in southeast Romania, which NATO allies also are using to get schooled on the fighter jets in 2020. But the delay is a window into the confusion and chaos that has confronted the military alliance’s rush to supply the A-16s\"\n",
    "#Write your code here\n",
    "first_answer = \n",
    "second_answer = \n",
    "third_answer = \n",
    "fourth_answer = \n",
    "print(first_answer)\n",
    "print(second_answer)\n",
    "print(fourth_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Functions\n",
    "\n",
    "- Function is a block of reusable codes. Annotation: y=f(x), where x is a list of input variables and y is a list of output variables.\n",
    "    - Terminology: input variables = <b>parameters</b>, output variables = <b>returned variables</b> and their actual values = <b>arguments</b>\n",
    "    - <b>Global vs Local</b>: function can create its local variables that are only used inside its boundary. Local variables can use same names as global variables without overriding their values.\n",
    "    - Format:\n",
    ">```python\n",
    "def function_name(input1[,input2,input3...]):\n",
    "        command line\n",
    "        return(output) \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The function of function is to transform x into y. Like a magic trick turning a girl into a tiger.\n",
    "\n",
    "<img src='img/week2-function.png' width='200px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mean() function without return values\n",
    "def mean(x):#x is a list of numbers\n",
    "    y = \n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the function output to a new variable a\n",
    "# Since the function does not define any output/return, a will be NaN\n",
    "a = mean([1,2,3])\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Revise the previous cell to add y as the return values to the function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revise mean() function to add return values\n",
    "mean([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mean([1,2,3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vs Local Variables\n",
    "# Variables with the same name will carry on different values within and outside the function\n",
    "x = mean([1,2,3])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a data_cleaning() function to convert letter case, remove punctuations, numbers, mentions, hashtags and hyperlinks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub('[0-9]+','',text)\n",
    "    text=re.sub('@[^ ]+','',text)\n",
    "    text=re.sub('#[^ ]+','',text)\n",
    "    text=re.sub('https:[^ ]+','',text)\n",
    "    text=re.sub('[\\W]+',' ',text)\n",
    "    text=text.strip()\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your function with a post from @realDonaldTrump\n",
    "a='@seanhannity “We the people will now be subjected to the biggest display of modern day McCarthyism....which is the widest fishing net expedition....every aspect of the presidents life....all in order to get power back so they can institute Socialism.” https://t.co/izb2tTrINB'\n",
    "data_cleaning(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenization\n",
    "- Definition: tokenization is a process of splitting sentences/paragraphs/documents into a set of words.\n",
    "- Differences in Languages:\n",
    "    - English: **words** are naturally separated with spaces\n",
    "    - Korean: **phrases** are naturally separated with spaces\n",
    "        - konlpy (http://konlpy.org/)\n",
    "    - Chinese/Japanese: **no spaces** in text\n",
    "        - Chinese: jieba (https://github.com/fxsjy/jieba)\n",
    "        - Japanese: jNlp (https://github.com/kevincobain2000/jProcessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize English Text: Hunt for Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the following sentence into words\n",
    "sentence='Mr. Zuckerberg, who runs Facebook, Instagram, WhatsApp and Messenger, on Wednesday expressed his intentions to change the essential nature of social media. Instead of encouraging public posts, he said he would focus on private and encrypted communications, in which users message mostly smaller groups of people they know. Unlike publicly shared posts that are kept as users’ permanent records, the communications could also be deleted after a certain period of time.'\n",
    "sentence=data_cleaning(sentence)\n",
    "words=sentence.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Second-run Data Cleaning\n",
    "- Main taks: remove stop words, stem/lemmatize words\n",
    "\n",
    "## 3.1 Remove stop words\n",
    "<font style=\"color:red\">English stop words file: https://juniorworld.github.io/python-workshop/doc/stop_words_eng.txt</font><br>\n",
    "Stop words are useless for understanding text.<br>\n",
    "- English: at, in, on, for, of, a, an, the...<br>\n",
    "- Chinese: 的，地，得，了.<br>\n",
    "\n",
    "However, the combination of 得了 is not a stop word.<br>\n",
    "-> Absolute Match\n",
    "\n",
    "Solution: Membership check - Check whether a word is in the predefined stopword list\n",
    ">```python\n",
    "x in [word1, word2, word3]\n",
    "x not in [word1, word2, word3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a' in ['a','b','c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a' in ['aa','b','c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a' not in ['aa','b','c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=open(\"./doc/stop_words_eng.txt\",'r')\n",
    "stopwords=[i.strip() for i in stopwords.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#go over every word in the list and check if it is a stop word\n",
    "#if not, add it to new list words_rm\n",
    "words_rm=[]\n",
    "for word in words:\n",
    "    #write your code here\n",
    "    \n",
    "words_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a user function called remove_stopwords()\n",
    "#Function Input: token list\n",
    "#Function Output: token list without stop words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remove_stopwords(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Stem/Lemmatize Words\n",
    "- Use external package `gensim` and `nltk` to achieve word stemming and lemmatization.\n",
    "- Words can have many variants and derivations. The goal of stemming and lemmatization is to convert the words back into their roots.\n",
    "- **Stem**: Crude. Trim off the suffixes and derivational affixes of words without knowledge of the context. Results are called stems.\n",
    "    - Advantage: Simple, Fast\n",
    "    - Disadvantage: Less accurate. Results are incomplete word roots. Cannot identify complex, context-based variants, such as comparative.\n",
    "    - Example: women -> women, apples -> appl, likely -> like, better -> better\n",
    "    >```python\n",
    "    from gensim.parsing.porter import PorterStemmer\n",
    "    Stemmer=PorterStemmer()\n",
    "    Stemmer.stem('word')\n",
    "    Stemmer.stem_documents(list_of_words)\n",
    "    ```\n",
    "- **Lemmatization**: Consider the context (part of speech of words) and converts the word to its meaningful base form, which is called Lemma.\n",
    "    - Advantage: Accurate and Contextualized. Results are meaningful, complete words.\n",
    "    - Disadvantage: Computationally Costly. Slow. Need to specify the part of speech. NLTK only supports lemmatizing nouns, adjs, and verbs.\n",
    "    - Example: women -> woman, apples -> apple, likely -> likely, is/are -> be, better -> good\n",
    "  \n",
    "    >```python\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    nltk.download('wordnet')\n",
    "    Lemmatizer=WordNetLemmatizer()\n",
    "    Lemmatizer.lemmatize('word','pos')\n",
    "    #pos = part of speech, 'n'=noun [default], 'a'=adjective, 'v'=verb, 'r'=adverbs\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install gensim\n",
    "! pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context=ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "Stemmer=PorterStemmer()\n",
    "Stemmer.stem('walking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Stemmer.stem('cats'),Stemmer.stem('apples'),Stemmer.stem('women'),Stemmer.stem('likely'),Stemmer.stem('is'),Stemmer.stem('better'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stemmer.stem_documents([\"cats\", \"apples\",\"women\",\"likely\",\"is\",\"better\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stem the words\n",
    "words_stem=Stemmer.stem_documents(words_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet') #only need to run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "Lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Lemmatizer.lemmatize('cats'),Lemmatizer.lemmatize('apples'),Lemmatizer.lemmatize('women'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Lemmatizer.lemmatize('likely','r'),Lemmatizer.lemmatize('is','v'),Lemmatizer.lemmatize('better','a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS is critical to results\n",
    "print(Lemmatizer.lemmatize('likely'),Lemmatizer.lemmatize('is'),Lemmatizer.lemmatize('better'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**<b>Reminder</b>** NLTK's lemmatizer is better used together with its nltk.pos_tag() function.<br>\n",
    "    The resulting tags are annotated according to: <a href=\"https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html\">https://cs.nyu.edu/~grishman/jet/guide/PennPOS.html</a><br>\n",
    "    POS of interest: Starting with 'N','J','V'</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.pos_tag(words_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(words_rm):\n",
    "    words_lemma=[]\n",
    "    for word,pos in nltk.pos_tag(words_rm):\n",
    "        if pos[0]=='N':\n",
    "            words_lemma.append(Lemmatizer.lemmatize(word))\n",
    "        elif pos[0]=='J':\n",
    "            words_lemma.append(Lemmatizer.lemmatize(word,'a'))\n",
    "        elif pos[0]=='V':\n",
    "            words_lemma.append(Lemmatizer.lemmatize(word,'v'))\n",
    "        else:\n",
    "            words_lemma.append(word)\n",
    "    return(words_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simply the above program using a dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_lemma=lemmatization(words_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download file from: https://juniorworld.github.io/python-workshop/doc/trump_tweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "table=pd.read_csv('https://juniorworld.github.io/python-workshop/doc/trump_tweets.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a program to:\n",
    "#Step 1: clean text\n",
    "#Step 2: tokenize text\n",
    "#Step 3: remove stop words from tokens\n",
    "#Step 4: lemmatize tokens\n",
    "#Step 5: save lemmas into csv file. tokens in the same tweet are saved at the same line\n",
    "#Step 6: combine all lemmas into a list\n",
    "lemmas_all=[]\n",
    "import csv\n",
    "file=open('trump_twitter_tokens.csv','w',newline='\\n')\n",
    "writer=csv.writer(file)\n",
    "for text in table['tweet']:\n",
    "    #Write your code here\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lemmas_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_freq=pd.Series(lemmas_all).value_counts().reset_index()\n",
    "lemmas_freq.columns=['word','freq']\n",
    "lemmas_freq[['freq','word']].to_csv('trump_wordcloud.csv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUIZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud\n",
    "HTML5 Word Cloud: http://timc.idv.tw/wordcloud/#\n",
    "\n",
    "### Word Co-occurence Network\n",
    "Download Gephi from: https://gephi.org/users/download/ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract mentions from his tweets\n",
    "# Generate a Co-Mention Network\n",
    "# Where nodes represent twitter handlers and links represent co-mention relationship in tweets\n",
    "# Hint: \n",
    "# Step 1: Check if the post has more than one mentions\n",
    "# Step 2: If yes, save mentions within a post into a row\n",
    "file=open('trump_twitter_comentions.csv','w',newline='\\n')\n",
    "writer=csv.writer(file)\n",
    "for text in table['tweet']:\n",
    "    #Write your code here\n",
    "    \n",
    "    \n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
